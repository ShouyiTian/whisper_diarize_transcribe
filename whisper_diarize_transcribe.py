import os
import subprocess
import datetime
import torch
from pydub import AudioSegment
from pyannote.audio import Pipeline
import whisper

# ==== é…ç½®é¡¹ ====
VIDEO_FILE = "meeting.mp4"
AUDIO_FILE = "meeting.wav"
CHUNKS_DIR = "chunks"
TEXT_OUTPUT = "transcription.txt"
SRT_OUTPUT = "transcription.srt"
LANGUAGE = "ja"
HUGGINGFACE_TOKEN = "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"  # <<< æ›¿æ¢ä¸ºä½ çš„ HuggingFace token
USE_GPU = torch.cuda.is_available()

# ==== Step 1: æå–éŸ³é¢‘ ====
if not os.path.exists(AUDIO_FILE):
    print(f"ğŸ¼ Step 1: æå–éŸ³é¢‘ {AUDIO_FILE} ...")
    subprocess.run(["ffmpeg", "-y", "-i", VIDEO_FILE, "-ar", "16000", "-ac", "1", AUDIO_FILE])
    print("âœ… éŸ³é¢‘æå–å®Œæˆ")

# ==== Step 2: è¯´è¯äººåˆ†ç¦» ====
print("ğŸ‘¥ Step 2: åŠ è½½ pyannote æ¨¡å‹å¹¶è¿›è¡Œè¯´è¯äººåˆ†ç¦» ...")
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=HUGGINGFACE_TOKEN)
if USE_GPU:
    pipeline.to(torch.device("cuda"))

diarization = pipeline(AUDIO_FILE)
segments = list(diarization.itertracks(yield_label=True))
num_speakers = len(set(s[2] for s in segments))
print(f"âœ… è¯†åˆ«åˆ° {num_speakers} ä½è¯´è¯äººï¼Œå…± {len(segments)} æ®µ")

# ==== Step 3: åˆ†å‰²éŸ³é¢‘æ®µ ====
print("âœ‚ï¸ Step 3: åˆ†å‰²éŸ³é¢‘æ®µè½ ...")
audio = AudioSegment.from_wav(AUDIO_FILE)
os.makedirs(CHUNKS_DIR, exist_ok=True)
chunk_files = []

for i, (segment, _, speaker) in enumerate(segments):
    start_ms = int(segment.start * 1000)
    end_ms = int(segment.end * 1000)
    chunk = audio[start_ms:end_ms]
    filename = f"{CHUNKS_DIR}/{i:03d}_{speaker}.wav"
    chunk.export(filename, format="wav")
    chunk_files.append((i, segment, speaker, filename))
print("âœ… åˆ†å‰²å®Œæˆ")

# ==== Step 4: Whisper è½¬å†™ ====
print("ğŸ§  Step 4: åŠ è½½ Whisper æ¨¡å‹ ...")
whisper_model = whisper.load_model("large", device="cuda" if USE_GPU else "cpu")

results = []
print("ğŸ“ Step 5: è½¬å†™æ¯æ®µéŸ³é¢‘ ...")
for i, segment, speaker, chunk_file in chunk_files:
    print(f"  â³ æ­£åœ¨è½¬å†™ç¬¬ {i+1}/{len(chunk_files)} æ®µï¼š{chunk_file}")
    result = whisper_model.transcribe(chunk_file, language=LANGUAGE)
    results.append((i, segment, speaker, result["text"].strip()))

# ==== Step 6: è¾“å‡ºæ–‡æœ¬ä¸ SRT ====
print("ğŸ“¤ Step 6: è¾“å‡ºè½¬å†™æ–‡æœ¬ä¸å­—å¹• ...")

with open(TEXT_OUTPUT, "w", encoding="utf-8") as txt_file, open(SRT_OUTPUT, "w", encoding="utf-8") as srt_file:
    for idx, (i, segment, speaker, text) in enumerate(results):
        start = str(datetime.timedelta(seconds=segment.start)).split(".")[0]
        end = str(datetime.timedelta(seconds=segment.end)).split(".")[0]

        # æ–‡æœ¬æ–‡ä»¶è¾“å‡º
        txt_file.write(f"[{start} --> {end}] {speaker}: {text}\n")

        # SRT æ–‡ä»¶è¾“å‡º
        def format_srt_time(t):
            td = datetime.timedelta(seconds=t)
            return str(td)[:11].replace(".", ",").rjust(12, "0")

        srt_file.write(f"{idx+1}\n")
        srt_file.write(f"{format_srt_time(segment.start)} --> {format_srt_time(segment.end)}\n")
        srt_file.write(f"{speaker}: {text}\n\n")

print(f"\nâœ… æ‰€æœ‰å†…å®¹ä¿å­˜å®Œæ¯•ï¼š\n- æ–‡æœ¬æ–‡ä»¶: {TEXT_OUTPUT}\n- å­—å¹•æ–‡ä»¶: {SRT_OUTPUT}")
